{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LdLCzV-5Eyr",
        "outputId": "0b1964c7-b1d5-48d7-abdc-371d630d5cb4"
      },
      "outputs": [],
      "source": [
        "#Chargement des datasets source\n",
        "import numpy as np\n",
        "import requests\n",
        "import io\n",
        "\n",
        "base_url = \"https://raw.githubusercontent.com/lmuxz/SCDA/master/data/\"\n",
        "suffixes = [\"test\", \"test_label\", \"train\", \"train_label\"]\n",
        "\n",
        "# Dictionnaire pour stocker les matrices numpy\n",
        "data_store_source = {}\n",
        "\n",
        "\n",
        "for suffix in suffixes:\n",
        "    file_name = f\"kaggle_source_cate_0_{suffix}.npy\"\n",
        "    url = f\"{base_url}{file_name}\"\n",
        "\n",
        "    response = requests.get(url)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        # numpy.load nécessite un objet \"file-like\", d'où l'utilisation de BytesIO\n",
        "        data_store_source[file_name] = np.load(io.BytesIO(response.content))\n",
        "        print(f\"Chargé : {file_name} | Forme : {data_store_source[file_name].shape}\")\n",
        "    else:\n",
        "        print(f\"Échec pour {file_name} (Code : {response.status_code})\")\n",
        "\n",
        "# Exemple d'accès :\n",
        "# train_data_0 = data_store['kaggle_source_cate_0_train.npy']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5VenjRzBg2J",
        "outputId": "463c1d95-bf55-44eb-9384-978ddf351194"
      },
      "outputs": [],
      "source": [
        "# Passage en dataframe pandas des datasets source :\n",
        "import pandas as pd\n",
        "\n",
        "def create_dataframes(data_dict):\n",
        "# 1. Extraction des matrices\n",
        "    x_train = data_dict['kaggle_source_cate_0_train.npy']\n",
        "    y_train = data_dict['kaggle_source_cate_0_train_label.npy']\n",
        "\n",
        "    x_test = data_dict['kaggle_source_cate_0_test.npy']\n",
        "    y_test = data_dict['kaggle_source_cate_0_test_label.npy']\n",
        "\n",
        "    # 2. Conversion des features en DataFrames\n",
        "    # On s'assure que les colonnes de features ont des noms clairs (ex: feat_0, feat_1...)\n",
        "    train_df = pd.DataFrame(x_train).add_prefix('feat_')\n",
        "    test_df = pd.DataFrame(x_test).add_prefix('feat_')\n",
        "\n",
        "    # 3. Conversion et intégration des labels (2 colonnes)\n",
        "    # On crée un DataFrame temporaire pour les labels avec des noms explicites\n",
        "    y_train_df = pd.DataFrame(y_train, columns=['label_0', 'label_1'], index=train_df.index)\n",
        "    y_test_df = pd.DataFrame(y_test, columns=['label_0', 'label_1'], index=test_df.index)\n",
        "\n",
        "    # 4. Concaténation horizontale (colonnes de features + colonnes de labels)\n",
        "    train_final = pd.concat([train_df, y_train_df], axis=1)\n",
        "    test_final = pd.concat([test_df, y_test_df], axis=1)\n",
        "\n",
        "    return train_final, test_final\n",
        "\n",
        "# Exécution\n",
        "train_df, test_df = create_dataframes(data_store_source)\n",
        "\n",
        "# Vérification de la structure\n",
        "print(f\"Colonnes disponibles : {list(train_df.columns)}\")\n",
        "print(f\"Forme finale Train : {train_df.shape}\") # Devrait être (n, features + 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJ1b7xQTjY2L"
      },
      "outputs": [],
      "source": [
        "X_train_source = train_df.drop(columns=['label_0', 'label_1'])\n",
        "y_train_source= train_df['label_1']\n",
        "\n",
        "X_test_source = test_df.drop(columns=['label_0', 'label_1'])\n",
        "y_test_source = test_df['label_1']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dddo9d64-gLg",
        "outputId": "3ab162be-f98c-4135-f3cd-6d9c3c59a9ec"
      },
      "outputs": [],
      "source": [
        "##Chargement des datasets cible\n",
        "\n",
        "base_url = \"https://raw.githubusercontent.com/lmuxz/SCDA/master/data/\"\n",
        "suffixes = [\"test\", \"test_label\", \"train\"]\n",
        "\n",
        "# Dictionnaire pour stocker les matrices numpy\n",
        "data_store_target = {}\n",
        "\n",
        "for i in range(4):\n",
        "    for suffix in suffixes:\n",
        "        file_name = f\"kaggle_target_cate_{i}_{suffix}.npy\"\n",
        "        url = f\"{base_url}{file_name}\"\n",
        "\n",
        "        response = requests.get(url)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            # numpy.load nécessite un objet \"file-like\", d'où l'utilisation de BytesIO\n",
        "            data_store_target[file_name] = np.load(io.BytesIO(response.content))\n",
        "            print(f\"Chargé : {file_name} | Forme : {data_store_target[file_name].shape}\")\n",
        "        else:\n",
        "            print(f\"Échec pour {file_name} (Code : {response.status_code})\")\n",
        "\n",
        "# Exemple d'accès :\n",
        "# train_data_0 = data_store['kaggle_source_cate_0_train.npy']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8ZPYnGeDHPy",
        "outputId": "aef0fe27-1976-4271-c1ad-19dc7360d809"
      },
      "outputs": [],
      "source": [
        " # Passage en dataframe pandas des datasets cible :\n",
        "all_train_target_dfs = []\n",
        "all_test_target_dfs = []\n",
        "datasets_target_par_index = {}\n",
        "\n",
        "for i in range(4):\n",
        "    print(f\"Traitement de l'index {i}...\")\n",
        "\n",
        "    # 1. Récupération\n",
        "    X_train_raw = data_store_target[f'kaggle_target_cate_{i}_train.npy']\n",
        "    X_test_raw  = data_store_target[f'kaggle_target_cate_{i}_test.npy']\n",
        "    y_test_raw  = data_store_target[f'kaggle_target_cate_{i}_test_label.npy']\n",
        "\n",
        "    # 2. Conversion dynamique\n",
        "    df_X_train = pd.DataFrame(X_train_raw).add_prefix('feat_')\n",
        "    df_X_test  = pd.DataFrame(X_test_raw).add_prefix('feat_')\n",
        "\n",
        "    # On crée les noms de colonnes dynamiquement pour les labels (label_0, label_1, etc.)\n",
        "    col_labels = [f'label_{j}' for j in range(y_test_raw.shape[1])]\n",
        "    df_y_test  = pd.DataFrame(y_test_raw, columns=col_labels)\n",
        "\n",
        "    # 3. Concaténation horizontale\n",
        "    df_train_full = df_X_train\n",
        "    df_test_full  = pd.concat([df_X_test, df_y_test], axis=1)\n",
        "\n",
        "    # Identification de la source\n",
        "    df_train_full['source_index'] = i\n",
        "    df_test_full['source_index'] = i\n",
        "\n",
        "    datasets_target_par_index[i] = {'train': df_train_full, 'test': df_test_full}\n",
        "    all_train_target_dfs.append(df_train_full)\n",
        "    all_test_target_dfs.append(df_test_full)\n",
        "\n",
        "# 4. Fusion finale\n",
        "df_final_train_target = pd.concat(all_train_target_dfs, ignore_index=True)\n",
        "df_final_test_target  = pd.concat(all_test_target_dfs, ignore_index=True)\n",
        "\n",
        "print(f\"\\nTerminé ! Colonnes créées pour les labels : {col_labels}\")\n",
        "print(f\"Format final du train : {df_final_train_target.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rOpQ2pJaj5pK"
      },
      "outputs": [],
      "source": [
        "X_train_target = df_final_train_target.drop(columns=['source_index'])\n",
        "X_test_target = df_final_test_target.drop(columns=['label_0', 'label_1', 'source_index'])\n",
        "y_test_target = df_final_test_target['label_1']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkoZa7DbFK3l",
        "outputId": "78a9cd96-04a4-4f72-d1f3-e029cd94bdfe"
      },
      "outputs": [],
      "source": [
        "print(df_final_train_target.head)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w3QQv4MzQ6rA"
      },
      "outputs": [],
      "source": [
        "#Procédure UDA\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def perform_domain_adaptation(X_source, y_source, X_target,X_eval, y_target_oracle):\n",
        "    \"\"\"\n",
        "    Implémente l'adaptation de domaine par re-pondération d'importance et correction locale k-NN.\n",
        "    \"\"\"\n",
        "\n",
        "    # --- 1. Calcul des poids d'importance via Classifieur de Domaine ---\n",
        "    # On crée un dataset pour apprendre à distinguer Source (0) de Cible (1)\n",
        "    X_domain = np.vstack([X_source, X_target])\n",
        "    y_domain = np.hstack([np.zeros(len(X_source)), np.ones(len(X_target))])\n",
        "\n",
        "    domain_clf = RandomForestClassifier(n_estimators=100, max_depth=6, random_state=42)\n",
        "    domain_clf.fit(X_domain, y_domain)\n",
        "\n",
        "    # Probabilité d'appartenir à la cible P(Target|x)\n",
        "    probs = domain_clf.predict_proba(X_source)[:, 1]\n",
        "\n",
        "    # Poids d'importance w(x) = P(Target|x) / P(Source|x)\n",
        "    # On ajoute un epsilon pour éviter la division par zéro\n",
        "    weights_global = probs / (1 - probs + 1e-6)\n",
        "\n",
        "    # --- 2. Affinement local par k-NN ---\n",
        "    # On cherche à voir si un point source est \"entouré\" de points cibles\n",
        "    knn_source = NearestNeighbors(n_neighbors=5).fit(X_source)\n",
        "    distances, indices = knn_source.kneighbors(X_target)\n",
        "\n",
        "    # On augmente le poids des points source qui sont les plus proches voisins de la cible\n",
        "    local_counts = np.zeros(len(X_source))\n",
        "    for idx_list in indices:\n",
        "        local_counts[idx_list] += 1\n",
        "\n",
        "    weights_local = local_counts / np.max(local_counts + 1e-6)\n",
        "\n",
        "    # --- 3. Combinaison des poids et Normalisation ---\n",
        "    # Fusion des approches globale (densité) et locale (voisinage)\n",
        "    final_weights = weights_global * (1 + weights_local)\n",
        "    final_weights = final_weights / np.mean(final_weights) # Normalisation\n",
        "\n",
        "    # --- 4. Entraînement du modèle XGBoost re-pondéré ---\n",
        "    # Note : On combine le class_weighting précédent avec les poids d'adaptation\n",
        "    # via le paramètre sample_weight de la méthode fit()\n",
        "\n",
        "    # Calcul du ratio pour le déséquilibre de classe (calculé sur Source)\n",
        "    ratio = np.bincount(y_source)[0] / np.bincount(y_source)[1]\n",
        "\n",
        "    model = XGBClassifier(\n",
        "        learning_rate=0.1,\n",
        "        max_depth=9,\n",
        "        scale_pos_weight=ratio,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # Entraînement sur Source avec les poids d'adaptation de domaine\n",
        "    model.fit(X_source, y_source, sample_weight=final_weights)\n",
        "\n",
        "    # --- 5. Évaluation \"Oracle\" sur le Domaine Cible ---\n",
        "    y_pred_target = model.predict(X_eval)\n",
        "\n",
        "    print(\"--- Rapport de Performance Oracle (Domaine Cible) ---\")\n",
        "    print(classification_report(y_target_oracle, y_pred_target))\n",
        "\n",
        "    return model\n",
        "\n",
        "# Appel de la fonction (exemple théorique)\n",
        "# model_adapted = perform_domain_adaptation(X_src, y_src, X_tgt, y_tgt_oracle)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16OeBtS0lQcs",
        "outputId": "57ae7690-92a5-4263-8f73-ff3d484062cb"
      },
      "outputs": [],
      "source": [
        "model_adapted = perform_domain_adaptation(X_source = X_train_source,\n",
        "    y_source = y_train_source,\n",
        "    X_target = X_train_target,\n",
        "    X_eval = X_test_target,\n",
        "    y_target_oracle = y_test_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hr41NbhipRpe",
        "outputId": "53d9a91c-e08b-4f19-ecbc-f0a9e0fd46b6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.metrics import recall_score, precision_score, f1_score\n",
        "import scipy.linalg\n",
        "# --- Initialisation des listes pour le tableau ---\n",
        "noms_methodes = []\n",
        "recalls = []\n",
        "precisions = []\n",
        "f1_scores = []\n",
        "\n",
        "# --- Fonction utilitaire pour enregistrer les scores ---\n",
        "def evaluer_et_stocker(nom, y_pred, y_true):\n",
        "    noms_methodes.append(nom)\n",
        "    recalls.append(recall_score(y_true, y_pred))\n",
        "    precisions.append(precision_score(y_true, y_pred))\n",
        "    f1_scores.append(f1_score(y_true, y_pred))\n",
        "\n",
        "# ==========================================\n",
        "# 1. FONCTIONS D'ADAPTATION\n",
        "# ==========================================\n",
        "\n",
        "def compute_importance_weights(X_src, X_tgt):\n",
        "    \"\"\"Calcule les poids d'importance (Global + Local)\"\"\"\n",
        "    # Global\n",
        "    X_dom = np.vstack([X_src, X_tgt])\n",
        "    y_dom = np.hstack([np.zeros(len(X_src)), np.ones(len(X_tgt))])\n",
        "    clf = RandomForestClassifier(n_estimators=100, max_depth=6, random_state=42).fit(X_dom, y_dom)\n",
        "    probs = clf.predict_proba(X_src)[:, 1]\n",
        "    w_global = probs / (1 - probs + 1e-6)\n",
        "\n",
        "    # Local\n",
        "    knn = NearestNeighbors(n_neighbors=5).fit(X_src)\n",
        "    _, indices = knn.kneighbors(X_tgt)\n",
        "    counts = np.zeros(len(X_src))\n",
        "    for idx in indices: counts[idx] += 1\n",
        "    w_local = counts / (np.max(counts) + 1e-6)\n",
        "\n",
        "    final_w = w_global * (1 + w_local)\n",
        "    return final_w / np.mean(final_w)\n",
        "\n",
        "def coral_alignment(X_src, X_tgt):\n",
        "    \"\"\"Aligne la covariance de la source sur celle de la cible\"\"\"\n",
        "    # Centrage des données\n",
        "    X_src_c = X_src - np.mean(X_src, axis=0)\n",
        "    X_tgt_c = X_tgt - np.mean(X_tgt, axis=0)\n",
        "\n",
        "    # Calcul des matrices de covariance\n",
        "    cov_src = np.cov(X_src_c, rowvar=False) + np.eye(X_src.shape[1])\n",
        "    cov_tgt = np.cov(X_tgt_c, rowvar=False) + np.eye(X_tgt.shape[1])\n",
        "\n",
        "    # Transformation CORAL : X_src_new = X_src * cov_src^{-1/2} * cov_tgt^{1/2}\n",
        "    inv_sqrt_src = scipy.linalg.inv(scipy.linalg.sqrtm(cov_src))\n",
        "    sqrt_tgt = scipy.linalg.sqrtm(cov_tgt)\n",
        "\n",
        "    X_src_coral = np.real(X_src_c @ inv_sqrt_src @ sqrt_tgt)\n",
        "    return X_src_coral + np.mean(X_tgt, axis=0)\n",
        "\n",
        "# ==========================================\n",
        "# 2. PRÉPARATION\n",
        "# ==========================================\n",
        "\n",
        "# Conversion en arrays NumPy (si nécessaire)\n",
        "X_train_source_np = np.array(X_train_source) if isinstance(X_train_source, pd.DataFrame) else X_train_source\n",
        "X_train_target_np = np.array(X_train_target) if isinstance(X_train_target, pd.DataFrame) else X_train_target\n",
        "X_test_target_np = np.array(X_test_target) if isinstance(X_test_target, pd.DataFrame) else X_test_target\n",
        "\n",
        "# A. Calcul des composants UDA\n",
        "weights_uda = compute_importance_weights(X_train_source_np, X_train_target_np)\n",
        "X_train_source_coral = coral_alignment(X_train_source_np, X_train_target_np)\n",
        "\n",
        "# Paramètres XGBoost\n",
        "params = {\n",
        "    'learning_rate': 0.1, 'max_depth': 9, 'random_state': 42,\n",
        "    'scale_pos_weight': np.sum(y_train_source == 0) / np.sum(y_train_source == 1)\n",
        "}\n",
        "\n",
        "# ==========================================\n",
        "# ENTRAÎNEMENT ET ÉVALUATION\n",
        "# ==========================================\n",
        "\n",
        "# 1. Baseline\n",
        "m0 = XGBClassifier(**params).fit(X_train_source_np, y_train_source)\n",
        "evaluer_et_stocker('Baseline', m0.predict(X_test_target_np), y_test_target)\n",
        "\n",
        "# 2. Re-weighting (Poids w(x))\n",
        "m1 = XGBClassifier(**params).fit(X_train_source_np, y_train_source, sample_weight=weights_uda)\n",
        "evaluer_et_stocker('UDA (Re-weighting)', m1.predict(X_test_target_np), y_test_target)\n",
        "\n",
        "# 3. CORAL (Alignement des features)\n",
        "m2 = XGBClassifier(**params).fit(X_train_source_coral, y_train_source)\n",
        "evaluer_et_stocker('UDA (CORAL)', m2.predict(X_test_target_np), y_test_target)\n",
        "\n",
        "# 4. Combinée\n",
        "m3 = XGBClassifier(**params).fit(X_train_source_coral, y_train_source, sample_weight=weights_uda)\n",
        "evaluer_et_stocker('UDA (Combinée)', m3.predict(X_test_target_np), y_test_target)\n",
        "# Pour CORAL, transformer aussi le test set\n",
        "X_test_target_coral = coral_alignment(X_test_target_np, X_train_target_np)\n",
        "\n",
        "# 3. CORAL (Alignement des features)\n",
        "m2 = XGBClassifier(**params).fit(X_train_source_coral, y_train_source)\n",
        "evaluer_et_stocker('UDA (CORAL)', m2.predict(X_test_target_coral), y_test_target)\n",
        "\n",
        "# 4. Combinée\n",
        "m3 = XGBClassifier(**params).fit(X_train_source_coral, y_train_source, sample_weight=weights_uda)\n",
        "evaluer_et_stocker('UDA (Combinée)', m3.predict(X_test_target_coral), y_test_target)\n",
        "\n",
        "# ==========================================\n",
        "# CRÉATION DU DATAFRAME \n",
        "# ==========================================\n",
        "\n",
        "df_comp = pd.DataFrame({\n",
        "    'Méthode': noms_methodes,\n",
        "    'Recall': recalls,\n",
        "    'Précision': precisions,\n",
        "    'F1-Score': f1_scores\n",
        "})\n",
        "\n",
        "print(df_comp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "m02fEay10N2r",
        "outputId": "02276fb1-406e-4347-8b25-482922aec30d"
      },
      "outputs": [],
      "source": [
        "print(df_comp.to_latex(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Obtenir les probabilités sur le domaine cible\n",
        "# [:, 1] permet de ne récupérer que la probabilité de la classe 1 (fraude)\n",
        "probs_baseline = m1.predict_proba(X_test_target)[:, 1]\n",
        "probs_coral = m2.predict_proba(X_test_target_coral)[:, 1]\n",
        "\n",
        "# 2. Vérifier que y_test_target est bien au bon format (vecteur 1D)\n",
        "# S'il a deux colonnes (label_0, label_1), ne prendre que la colonne 1\n",
        "if len(y_test_target.shape) > 1 and y_test_target.shape[1] == 2:\n",
        "    y_true_plot = y_test_target[:, 1]\n",
        "else:\n",
        "    y_true_plot = y_test_target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import precision_recall_curve, auc\n",
        "\n",
        "# Calcul des courbes\n",
        "p_base, r_base, _ = precision_recall_curve(y_test_target, probs_baseline)\n",
        "p_coral, r_coral, _ = precision_recall_curve(y_test_target, probs_coral)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(r_base, p_base, label=f'Source-only (AUC-PR = {auc(r_base, p_base):.2f})', color='red', linestyle='--')\n",
        "plt.plot(r_coral, p_coral, label=f'Adaptation CORAL (AUC-PR = {auc(r_coral, p_coral):.2f})', color='blue', linewidth=2)\n",
        "\n",
        "plt.xlabel('Rappel (Recall)')\n",
        "plt.ylabel('Précision (Precision)')\n",
        "plt.title('Impact de CORAL sur la courbe Précision-Rappel (Domaine Cible)')\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "fraudapt",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
